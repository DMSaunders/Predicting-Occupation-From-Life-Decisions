Predicting label 15 with 3 edu features

Performing model optimizations...

Estimator: Logistic Regression w/ L1
Fitting 10 folds for each of 6 candidates, totalling 60 fits
Best params: {'clf__C': 0.1, 'clf__class_weight': None, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}
Best training f1: 0.968
Test set f1 score for best params: 0.398 

Estimator: LogisticRegression w/ L2
Fitting 10 folds for each of 18 candidates, totalling 180 fits
Best params: {'clf__C': 1.0, 'clf__class_weight': None, 'clf__penalty': 'l2', 'clf__solver': 'newton-cg'}
Best training f1: 0.968
Test set f1 score for best params: 0.404 

Estimator: SGDClassifier
Fitting 10 folds for each of 72 candidates, totalling 720 fits
[CV] clf__alpha=0.1, clf__class_weight=None, clf__loss=hinge, clf__penalty=l1
Best params: {'clf__alpha': 0.001, 'clf__class_weight': None, 'clf__loss': 'log', 'clf__penalty': 'l1'}
Best training f1: 0.966
Test set f1 score for best params: 0.409 



Estimator: DecisionTreeClassifier
Fitting 10 folds for each of 1440 candidates, totalling 14400 fits
Best params: {'clf__class_weight': None, 'clf__criterion': 'entropy', 'clf__max_depth': 1000, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 10}
Best training f1: 0.968
Test set f1 score for best params: 0.417
test auc: 

Estimator: Random Forest
Fitting 10 folds for each of 2160 candidates, totalling 21600 fits
Best params: {'clf__class_weight': None, 'clf__criterion': 'entropy', 'clf__max_depth': 1000, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 10}
Best training f1: 0.968
Test set f1 score for best params: 0.417 
test auc 0.815653444124108:                

Estimator: Random Forest w/ Scaling
Fitting 10 folds for each of 2160 candidates, totalling 21600 fits
Best params: {'clf__class_weight': None, 'clf__criterion': 'entropy', 'clf__max_depth': 1000, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 10}
Best training f1: 0.968
Test set f1 score for best params: 0.417


GradientBoosting
done manually. (max_depth=2)
test f1 of 0.396
test accuracy 96.66%

Estimator: SVC
Fitting 10 folds for each of 1 candidates, totalling 10 fits
Best params: {}
Best training f1: 0.967
Test set f1 score for best params: 0.393 

Estimator: KNeighborsClassifier
Fitting 10 folds for each of 10 candidates, totalling 100 fits
Best params: {'clf__n_neighbors': 7}
Best training f1: 0.967
Test set f1 score for best params: 0.381 

XGBoost
Best params: {}
Best training f1: 0.968
test accuracy 96.697%    
Test set f1 score for best params: 0.393 

----------pca 2

Estimator: Logistic Regression w/ L1 pca
Fitting 10 folds for each of 6 candidates, totalling 60 fits
Best params: {'clf__C': 0.5, 'clf__class_weight': None, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}
Best training f1: 0.964
Test set f1 score for best params: 0.027

Estimator: LogisticRegression w/ L2 pca
Fitting 10 folds for each of 18 candidates, totalling 180 fits
Best params: {'clf__C': 0.1, 'clf__class_weight': None, 'clf__penalty': 'l2', 'clf__solver': 'newton-cg'}
Best training f1: 0.964
Test set f1 score for best params: 0.000

Estimator: SGDClassifier pca
Fitting 10 folds for each of 72 candidates, totalling 720 fits
Best params: {'clf__alpha': 0.1, 'clf__class_weight': None, 'clf__loss': 'hinge', 'clf__penalty': 'l1'}
Best training f1: 0.964
Test set f1 score for best params: 0.000

Estimator: Random Forest pca scaling
Fitting 10 folds for each of 2160 candidates, totalling 21600 fits
Best params: {'clf__class_weight': None, 'clf__criterion': 'gini', 'clf__max_depth': 100, 'clf__min_samples_leaf': 9, 'clf__min_samples_split': 3}
Best training f1: 0.966
Test set f1 score for best params: 0.072

---pca 20

Estimator: Logistic Regression w/ L1 pca
Fitting 10 folds for each of 6 candidates, totalling 60 fits
Best params: {'clf__C': 0.5, 'clf__class_weight': None, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}
Best training f1: 0.965
Test set f1 score for best params: 0.203 

-- pca 40

Estimator: Logistic Regression w/ L1 pca
Fitting 10 folds for each of 6 candidates, totalling 60 fits
Best params: {'clf__C': 0.5, 'clf__class_weight': None, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}
Best training f1: 0.965
Test set f1 score for best params: 0.207 


----------------------------------------------
rf

accuracy_score(y_train, y_pred_all_0)
0.9640853388454156

f1_score(y_train, y_pred_mostly_0) # can't have all 0 in y_pred so added a single 1
0.0

accuracy_score(y_test, y_pred) # real accuracy score on rf model
0.9666264748944966

improvement of like 7% of the possible improvement





Best params: {'clf__class_weight': None, 'clf__criterion': 'entropy', 'clf__max_depth': 1000, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 10}
Best training f1: 0.968


-----
all maj occp

Estimator: DecisionTreeClassifier untuned
train f1 0.24151040897681972
test f1 0.21212643183188357



---------------- freewill features
untuned XGBoost
Best params: {}
Best training f1: 0.969
test accuracy 96.938%                #my accuracy improved but f1 decreased???
Test set f1 score for best params: 0.394

untuned GradientBoosting
Best params: {}
Best training f1: 0.969
test accuracy 0.9690810438377401:
Test set f1 score for best params: 0.415

untuned rf on computer occp only
Best params: {}
Best training f1: 0.969
test accuracy 0.9688226681595039:   
test auc 0.7809562614722637:  I'd been doing these wrong
Test set f1 score for best params: 0.359 

tuned rf on occp 15
Best params: {'clf__criterion': 'entropy', 'clf__max_depth': 100, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 10, 'clf__n_estimators': 200}
Best training f1: 0.969
test accuracy 0.9691671690638188: <----------------best
Test set f1 score for best params: 0.372 
test auc 0.8660411706311473    <------------------ best

------------------------------------------------------------------------------------------------------
SSH into jupyter

ssh -i ~/.ssh/first_key.pem ubuntu@ec2-52-206-68-215.compute-1.amazonaws.com
jupyter notebook --no-browser --port=8888

ssh -i ~/.ssh/first_key.pem -L 8157:127.0.0.1:8888 ubuntu@ec2-52-90-67-198.compute-1.amazonaws.com
Navigate to http://127.0.0.1:8157 in your browser
Use     Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:3333/?token=            739964ce5438e15829ab2d5e155819dba1172fb2af77fad9          &token=739964ce5438e15829ab2d5e155819dba1172fb2af77fad9


[ec2-user ~]$ git clone https://github.com/DMSaunders/capstone
pip install joblib
pip install xgboost

fd5d606b5c8ab8c918e682bcf35bf4ecbab79746b0e9d358&token=fd5d606b5c8ab8c918e682bcf35bf4ecbab79746b0e9d358

scp -i ~/.ssh/first_key.pem ~/galv/capstone/notebooks/freewill_df.csv ubuntu@ec2-18-234-116-27.compute-1.amazonaws.com:~/capstone/notebooks